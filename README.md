# Ikaris Assistant ğŸ¦¾ (v1.2.1)

A hyper-personalized local AI research assistant powered by **Hydra**, **LangGraph**, and **Sherpa-ONNX** â€” a full local multimodal research agent with hybrid RAG, comparative synthesis, and NPU-accelerated voice.

> [!IMPORTANT]
> **Release Version**: v1.2.1
> This version introduces Audio v2: Silero VAD speech gating, live partial hypothesis display, STT confidence scoring, and automatic NPUâ†’CPU fallback. Built on top of v1.2.0's Hydra configuration, hybrid RAG, and Sherpa-ONNX audio stack.

## ğŸš€ Key Features

- **Hydra Configuration**: Declarative YAML configs for models, tools, paths, audio, and hardware. Switch anything with a single CLI flag.
- **Dependency-Injected Agent**: Clean `Agent(llm, tools, audio)` architecture â€” no global state, fully modular.
- **Hybrid RAG**: Combines local FAISS (PDF) retrieval with PubMed biomedical search. Capability-routed: biomedical queries automatically engage PubMed.
- **Comparative Synthesis**: Multi-source evidence is analyzed for consensus, conflicts, and research gaps â€” not just retrieved.
- **Unified Evidence Layer**: All retrieval sources emit standardized `Evidence` dataclass objects for deduplication and ranking.
- **Sherpa-ONNX Audio Stack**: NPU-accelerated voice I/O with automatic fallbacks:

  | Profile | STT | TTS | Provider |
  |---------|-----|-----|----------|
  | `npu` | Zipformer Streaming | Kokoro (82M) | OpenVINO â†’ Intel NPU |
  | `cuda` | Whisper float16 | Kokoro (82M) | CUDA â†’ GPU |
  | `cpu` | Whisper INT8 | Piper VITS | CPU fallback |
  | `none` | Disabled | Disabled | Text-only |

- **Local Brain**: Powered by LM Studio or Ollama (OpenAI-compatible).
- **Intelligent Routing**: Graph-based router dispatches to hardware stats, research, Logseq, or general chat.
- **Hardware Monitoring**: Real-time CPU and battery stats (optimized for ROG Strix G16).
- **Persistent Memory**: Conversation history across restarts via SQLite.
- **Logseq Sync**: Auto-logs research insights and retrieves handwritten notes.
- **Offline First**: Aggressive model caching for fully offline usage.
- **Cross-Platform**: Path configs for Linux (ROG Strix), macOS, and Windows.

## ğŸ—ï¸ Architecture

```
Hydra (configs/)
 â”œâ”€â”€ model   (lm_studio / ollama)
 â”œâ”€â”€ tools   (faiss / pubmed / logseq / research)
 â”œâ”€â”€ audio   (npu / cuda / cpu / none)
 â””â”€â”€ paths   (strix / linux / mac / windows)
         â”‚
         â–¼
   Agent(llm, tools, audio)
         â”‚
         â–¼
   StateGraph (LangGraph)
         â”‚
   START â†’ summarize â†’ router
                         â”œâ”€â”€ hardware_node â†’ END
                         â”œâ”€â”€ llm_node â†’ END
                         â”œâ”€â”€ logseq_node â†’ END
                         â”œâ”€â”€ research_node â†’ synthesis_node â†’ END
                         â””â”€â”€ agent_planning_node
                               â””â”€â”€ retrieval_node (FAISS + PubMed hybrid)
                                     â””â”€â”€ reasoning_node
                                           â”œâ”€â”€ retrieval_node (loop)
                                           â””â”€â”€ generate_answer_node â†’ END
         â”‚
   Mic (STT) â†â†’ UI â†â†’ Speaker (TTS)
```

## ğŸ›  Project Structure

```text
ikaris_assistant/
â”œâ”€â”€ configs/                    # Hydra configuration hierarchy
â”‚   â”œâ”€â”€ main.yaml               # Master config switch
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ lm_studio.yaml      # LM Studio backend
â”‚   â”‚   â””â”€â”€ ollama.yaml         # Ollama backend
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ npu.yaml            # Intel NPU (Zipformer + Kokoro)
â”‚   â”‚   â”œâ”€â”€ cuda.yaml           # NVIDIA GPU (Whisper + Kokoro)
â”‚   â”‚   â”œâ”€â”€ cpu.yaml            # CPU fallback (Whisper INT8 + Piper)
â”‚   â”‚   â””â”€â”€ none.yaml           # Text-only (disabled)
â”‚   â”œâ”€â”€ paths/
â”‚   â”‚   â”œâ”€â”€ strix.yaml          # ROG Strix G16 (Linux)
â”‚   â”‚   â”œâ”€â”€ linux.yaml          # Generic Linux
â”‚   â”‚   â”œâ”€â”€ mac.yaml            # macOS
â”‚   â”‚   â””â”€â”€ windows.yaml        # Windows
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ paper.yaml          # FAISS PDF tool
â”‚   â”‚   â”œâ”€â”€ research.yaml       # ArXiv downloader tool
â”‚   â”‚   â”œâ”€â”€ logseq.yaml         # Logseq notes tool
â”‚   â”‚   â””â”€â”€ pubmed.yaml         # PubMed biomedical tool
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ default.yaml        # UI theme config
â”‚
â”œâ”€â”€ models/                     # ONNX model weights (gitignored)
â”‚   â”œâ”€â”€ stt/                    # Speech-to-Text models
â”‚   â”‚   â””â”€â”€ README.md           # Download instructions
â”‚   â”œâ”€â”€ tts/                    # Text-to-Speech models
â”‚   â”‚   â””â”€â”€ README.md           # Download instructions
â”‚   â””â”€â”€ vad/                    # Voice Activity Detection
â”‚       â””â”€â”€ README.md           # Download instructions
â”‚
â”œâ”€â”€ run.py                      # Hydra-powered entry point (GUI + CLI)
â”œâ”€â”€ run_cli.py                  # DEPRECATED â€” use `python run.py mode=cli`
â”œâ”€â”€ papers/                     # Drop your research PDFs here
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ agent.py                # Agent class (DI-based graph builder)
    â”œâ”€â”€ evidence.py             # Unified Evidence dataclass
    â”œâ”€â”€ main.py                 # Node definitions, routing, GUI bootstrap
    â”œâ”€â”€ state.py                # LangGraph State definition
    â”œâ”€â”€ nodes/
    â”‚   â”œâ”€â”€ llm_node.py             # General chat node
    â”‚   â”œâ”€â”€ reasoning_node.py       # Agentic evidence evaluator
    â”‚   â”œâ”€â”€ research_node.py        # ArXiv batch download node
    â”‚   â”œâ”€â”€ retrieval_node.py       # Hybrid retrieval (FAISS + PubMed)
    â”‚   â””â”€â”€ synthesis_node.py       # Comparative synthesis engine
    â”œâ”€â”€ tools/
    â”‚   â”œâ”€â”€ hardware.py             # System stats
    â”‚   â”œâ”€â”€ paper_tool.py           # FAISS vector search (PaperTool)
    â”‚   â”œâ”€â”€ research_tool.py        # ArXiv downloader (ResearchTool)
    â”‚   â”œâ”€â”€ logseq_tool.py          # Logseq journal (LogseqTool)
    â”‚   â””â”€â”€ pubmed_tool.py          # PubMed biomedical (PubMedTool)
    â”œâ”€â”€ ui/
    â”‚   â”œâ”€â”€ main_window.py          # PyQt5 researcher interface
    â”‚   â”œâ”€â”€ chat_widget.py          # Chat panel
    â”‚   â”œâ”€â”€ sidebar_widget.py       # Paper sidebar
    â”‚   â”œâ”€â”€ status_bar.py           # System status bar
    â”‚   â”œâ”€â”€ styles.py               # Dark theme
    â”‚   â””â”€â”€ workers.py              # Background QThread workers
    â””â”€â”€ utils/
        â”œâ”€â”€ audio.py                # SherpaAudioStack (STT + TTS engine)
        â”œâ”€â”€ voice.py                # Legacy faster-whisper (fallback)
        â”œâ”€â”€ helpers.py              # Hardware detection
        â”œâ”€â”€ instantiators.py        # Hydra instantiation glue
        â”œâ”€â”€ llm_client.py           # LLM client functions
        â””â”€â”€ summarizer.py           # Conversation compressor
```

## âš™ï¸ Setup

### 1. Environment
```bash
conda env create -f environment.yml
conda activate ikaris_env
pip install -r requirements.txt
```

### 2. Local LLM Server
Start **LM Studio** or **Ollama** and ensure the local server is running:
- LM Studio: `http://localhost:1234/v1`
- Ollama: `http://localhost:11434/v1`

### 3. Research Papers
Place PDFs in `ikaris_assistant/papers/`. Auto-indexed on first query.

### 4. PubMed (Optional)
```bash
export NCBI_API_KEY=your_real_key_here
```

### 5. Audio Models (Sherpa-ONNX)

Install Sherpa-ONNX:
```bash
pip install sherpa-onnx
```

Download models into `models/stt/` and `models/tts/`:
- See `models/stt/README.md` and `models/tts/README.md` for links.

**Validate your setup before Python:**
```bash
# Test STT (replace with your model paths)
sherpa-onnx-offline-asr \
  --nn-model=./models/stt/whisper-base-int8.onnx \
  --tokens=./models/stt/tokens.txt \
  --provider=openvino \
  --device=NPU

# If this works â†’ Python will work.
```

## ğŸ¤ Usage

### Run the GUI (default)
```bash
python run.py
```

### Run in CLI mode (no GUI, terminal REPL)
```bash
python run.py mode=cli
```
Inside the CLI you can type messages directly, press `v` + Enter for voice input, or `exit` to quit.

### Combine CLI with other overrides
```bash
python run.py mode=cli audio=none          # text-only terminal
python run.py mode=cli model=ollama        # use Ollama backend
python run.py mode=cli audio=cuda paths=linux
```

### Override configs via CLI (Hydra)
```bash
# Switch LLM backend
python run.py model=ollama

# Switch audio profile
python run.py audio=npu          # Intel NPU (Zipformer streaming + Kokoro)
python run.py audio=cuda         # NVIDIA GPU (Whisper float16 + Kokoro)
python run.py audio=cpu          # CPU fallback (Whisper INT8 + Piper)
python run.py audio=none         # Text-only, no mic

# Switch platform paths
python run.py paths=mac
python run.py paths=windows
python run.py paths=linux

# Use CPU instead of GPU for inference
python run.py device=cpu

# Combine overrides
python run.py model=ollama audio=cuda paths=linux

# Print resolved config (debugging)
python run.py --cfg job
```

### Chat Commands
- **General Chat**: Just type or speak.
- **Hardware**: Ask about "battery" or "cpu".
- **Research**: Ask questions about your papers (e.g., "What is Scaled Dot-Product Attention?").
- **Download Papers**: Paste ArXiv IDs (e.g., "download 1706.03762 2307.09288").
- **Logseq**: Every research insight is logged to your Logseq journal.
- **Biomedical**: Queries with medical/biological terms auto-trigger PubMed hybrid search.
- **Voice**: Click the mic button or use the voice shortcut (requires `audio=npu|cuda|cpu`).

## ğŸ”Š Audio Profiles

| Profile | STT Engine | TTS Engine | Best For |
|---------|-----------|-----------|----------|
| **npu** | Zipformer (streaming, low-latency) | Kokoro 82M | Intel Core Ultra laptops with NPU |
| **cuda** | Whisper small (float16, high accuracy) | Kokoro 82M | NVIDIA GPU systems (RTX 5070 Ti) |
| **cpu** | Whisper base (INT8, lightweight) | Piper VITS | Any system, no GPU needed |
| **none** | Disabled | Disabled | Text-only, headless, CI/testing |

### Why these choices?
- **Zipformer** is designed for low-latency streaming â€” perfect for always-on mic on NPU.
- **Whisper INT8** via OpenVINO can hit the NPU/CPU efficiently for batch transcription.
- **Kokoro** (82M) is small enough for NPU yet sounds premium.
- **Piper** is rock-solid fallback when Kokoro glitches.
- **Sherpa-ONNX + OpenVINO** is currently the best local stack for Intel NPU + privacy + latency.

## ğŸ§  Audio v2 Features

### 1. Voice Activity Detection (Silero VAD)
Silero VAD gates the microphone so STT only processes actual speech. This saves power, improves accuracy, and makes the UX snappier. Download the model:
```bash
bash scripts/pull_models.sh vad
```

### 2. Partial Hypothesis Display
Zipformer streaming STT emits partial tokens as you speak. The UI shows live transcription (`ğŸ¤ ... hello how are`) that updates in real-time. Feels magical.

### 3. Confidence Scoring
Every transcription returns a confidence score (0.0â€“1.0) extracted from token probabilities:
- ğŸŸ¢ â‰¥70% â€” high confidence
- ğŸŸ¡ â‰¥40% â€” medium confidence
- ğŸ”´ <40% â€” low confidence (consider asking user to repeat)

The confidence is stored in `IkarisState.stt_confidence` so downstream nodes can factor it in.

### 4. Auto-Switch STT
If the primary STT engine (NPU/CUDA) fails to load, the system automatically falls back to CPU Whisper INT8. No config change needed â€” the CPU models are already downloaded. The UI shows an âš¡ indicator when auto-switch occurs.

### v1.2.1 (Audio v2)
- **Silero VAD**: Voice Activity Detection gates microphone â€” no wasted compute on silence.
- **Partial Hypothesis**: Zipformer streaming emits live tokens to UI for real-time transcription display.
- **Confidence Scoring**: STT confidence (0.0â€“1.0) exposed to `IkarisState.stt_confidence` with ğŸŸ¢/ğŸŸ¡/ğŸ”´ badges.
- **Auto-Switch STT**: If primary provider (NPU/CUDA) fails, automatic fallback to CPU Whisper INT8.
- **VoiceWorker QThread**: Voice input runs in background thread â€” UI never freezes during recording.
- **STTResult dataclass**: Rich return type with text, confidence, duration, provider, and fallback status.

### v1.2.0
- **Hydra Integration**: Full declarative config system (`configs/` hierarchy).
- **Agent Class**: `Agent(llm, tools, audio)` â€” dependency injection replaces all global state.
- **Sherpa-ONNX Audio**: NPU/CUDA/CPU audio profiles with Zipformer streaming STT, Whisper offline STT, Kokoro TTS, and Piper TTS fallback.
- **Evidence Dataclass**: Unified retrieval layer across FAISS, PubMed, and Logseq.
- **Hybrid RAG**: FAISS + PubMed merge ranking with deduplication and capability routing.
- **PubMedTool**: Real biomedical literature search via metapub (ESearch + EFetch + FindIt).
- **Synthesis Node**: Comparative analysis across multi-source evidence.
- **Capability Routing**: Biomedical intent detection triggers PubMed automatically.
- **Cross-Platform Configs**: Path profiles for Linux, macOS, and Windows.
- **Ollama Backend**: Alternative LLM backend support.
- **Observability**: Tool call logging with latency tracking via Hydra outputs.

### v1.1.0
- Wayland stability fixes.
- Offline model caching.
- PyQt5 GUI with streaming tokens.
- Background PDF indexing.

---
*Built for the ROG Strix G16. Runs anywhere.*
